{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "In the lab, we saw how we can take a training script and use `mlflow` to submit runs. In practice, the training script isn't always clean code that is ready to use. So in this assignment, we learn to **refactor code** to train a model and prepare it for scoring. We learn about functionality in `sklearn` for **model persistence**, a fancy term for saving a model, so we can later load it and use it for prediction. We also learn how we can chain pre-processing steps and attach them to the model prediction step so we can both pre-process and score in one smooth flow.\n",
    "\n",
    "The bulk of the code that needs to execute is already given. This should look like code that we've written throughout the course. But when moving to production, it is **highly, highly recommended** that we **refactor** the code. What this means is that we need to go over the code from top to finish and do a bunch of things (now that we have the advantage of **hindsight**):\n",
    "\n",
    "- add **comments** in the code, for future us or (heaven forbid!) if someone else has to look at our code!\n",
    "- remove **extra code** that we wrote during development for debugging purposes but no longer need, or at least comment it out\n",
    "- simplify things, remove redundancies and make the code more **modular** by using functions or classes if needed\n",
    "- **parametrize** the code, so you avoid **hard-coding** things that need to change, and move them as high-level parameters at the top of the code, making it easy to change things without breaking things\n",
    "- create a runtime for the environment using Conda or PIP, which acts as a snapshot of the Python libraries we used and pins down their versions (careful: not all packages used during training are needed during scoring, and since the scoring enivironment is supposed to be **lightweight**, we should identify and remove such packages)\n",
    "- add **scaffolding**, this is to make sure that the code executes **gracefully** when errors happen, such as when the model expects a certain feature in the future data but it is missing for some reason (by gracefully, we mean that we use things like `try` and `except` to catch and redirect errors)\n",
    "- last but not least, never stop **testing**, but testing here can mean **unit testing**, **integration testing**, and even statistical tests for **data drift** or **model drift**\n",
    "- if you haven't yet (what have you been waiting for!) begin to **version control** your code using **git** or something similar\n",
    "\n",
    "Of course even with hindsight, doing these things is not easy and the answers are not always available, but we do the best we can and with experience we get better at it. Every project can be viewed as a work in progress, but applying certain **best practices** can make it easier to keep improving things without breaking them. This is what **agile development** is all about. You may have noticed that all the above steps are things we do generally when we write applications. It's just that not all data scientists have a rigorous computer science background and so we tend to have looser standards in general. The above is just the beginnig, not the end. Usually depending on the type of deployment we are doing, there are specific additional steps needed. \n",
    "\n",
    "Enough said. It's time for work! We used our knowledge of data science to write up some code to read data, pre-process it, train a model and use it to get predictions on a test data set. Here's a standard training code snippet. Examine it and run it to make sure it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns are age, balance, day, duration, campaign, pdays, previous.\n",
      "Categorical columns are job, marital, education, default, housing, loan, contact, month, poutcome.\n",
      "Training data has 40689 rows.\n",
      "Test data has 4522 rows.\n",
      "Featurized training data has 40689 rows and 51 columns.\n",
      "Featurized test data has 4522 rows and 51 columns.\n",
      "Precision = 65% and recall = 35% on the training data.\n",
      "Precision = 63% and recall = 34% on the validation data.\n"
     ]
    }
   ],
   "source": [
    "# frontload all of the library imports\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# set global state variable\n",
    "SEED = 42\n",
    "\n",
    "# read the csv into a dataframe\n",
    "bank = pd.read_csv('./bank-full.csv', delimiter = ';')\n",
    "\n",
    "# separate numeric, categorical and target columns\n",
    "num_cols = bank.select_dtypes(['integer', 'float']).columns\n",
    "cat_cols = bank.select_dtypes(['object']).drop(columns = \"y\").columns\n",
    "\n",
    "# print column types\n",
    "print(\"Numeric columns are {}.\".format(\", \".join(num_cols)))\n",
    "print(\"Categorical columns are {}.\".format(\", \".join(cat_cols)))\n",
    "\n",
    "# split the data into training and testing data as 90-10 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(bank.drop(columns = \"y\"), bank[\"y\"], \n",
    "                                                    test_size = 0.10, random_state = 42)\n",
    "# reset index of training and testing data\n",
    "X_train = X_train.reset_index(drop = True)\n",
    "X_test = X_test.reset_index(drop = True)\n",
    "\n",
    "# show the shape of the training and testing data\n",
    "print(\"Training data has {} rows.\".format(X_train.shape[0]))\n",
    "print(\"Test data has {} rows.\".format(X_test.shape[0]))\n",
    "\n",
    "#------------------------------------------------------------------------------------------#\n",
    "\n",
    "# One hot encode the categorical data\n",
    "onehotter = OneHotEncoder(sparse_output = False)\n",
    "onehotter.fit(X_train[cat_cols])\n",
    "onehot_cols = onehotter.get_feature_names_out(cat_cols)\n",
    "X_train_onehot = pd.DataFrame(onehotter.transform(X_train[cat_cols]), columns = onehot_cols)\n",
    "X_test_onehot = pd.DataFrame(onehotter.transform(X_test[cat_cols]), columns = onehot_cols)\n",
    "\n",
    "# Train normalizer using z-score normalization\n",
    "znormalizer = StandardScaler()\n",
    "znormalizer.fit(X_train[num_cols])\n",
    "\n",
    "# transform numeric columns \n",
    "X_train_norm = pd.DataFrame(znormalizer.transform(X_train[num_cols]), columns = num_cols)\n",
    "X_test_norm = pd.DataFrame(znormalizer.transform(X_test[num_cols]), columns = num_cols)\n",
    "\n",
    "# build out featurized dataframes\n",
    "X_train_featurized = X_train_onehot # add one-hot-encoded columns\n",
    "X_test_featurized = X_test_onehot   # add one-hot-encoded columns\n",
    "X_train_featurized[num_cols] = X_train_norm # add numeric columns\n",
    "X_test_featurized[num_cols] = X_test_norm   # add numeric columns\n",
    "\n",
    "# delete unneeded dataframes from memory\n",
    "del X_train_norm, X_test_norm, X_train_onehot, X_test_onehot\n",
    "\n",
    "# show infor about featurized dataframes to compare to non-featurized\n",
    "print(\"Featurized training data has {} rows and {} columns.\".format(*X_train_featurized.shape))\n",
    "print(\"Featurized test data has {} rows and {} columns.\".format(*X_test_featurized.shape))\n",
    "\n",
    "#------------------------------------------------------------------------------------------#\n",
    "\n",
    "# instantiate logistic regressor\n",
    "logit = LogisticRegression(max_iter = 5000, solver = 'lbfgs', random_state=SEED)\n",
    "logit.fit(X_train_featurized, y_train)\n",
    "\n",
    "y_hat_train = logit.predict(X_train_featurized) # predict training data\n",
    "y_hat_test = logit.predict(X_test_featurized) # predict testing data\n",
    "\n",
    "# Show precision and recall for both training and testing predictions\n",
    "precision_train = precision_score(y_train, y_hat_train, pos_label = 'yes') * 100\n",
    "precision_test = precision_score(y_test, y_hat_test, pos_label = 'yes') * 100\n",
    "\n",
    "recall_train = recall_score(y_train, y_hat_train, pos_label = 'yes') * 100\n",
    "recall_test = recall_score(y_test, y_hat_test, pos_label = 'yes') * 100\n",
    "\n",
    "print(\"Precision = {:.0f}% and recall = {:.0f}% on the training data.\".format(precision_train, recall_train))\n",
    "print(\"Precision = {:.0f}% and recall = {:.0f}% on the validation data.\".format(precision_test, recall_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above trains a model on data that contains both categorical and numeric features. We normalize the numeric features and one-hot-encode the categorical features as part of pre-processing. In the above code we do this \"manually\", however as shown [here](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer.html) we can compose data transformations and ML steps to create a **single multi-step pipeline**. The pipeline object (conviniently called `pipeline` in the docs) has a `fit` and `predict` method:\n",
    "- By calling `fit`, the raw input data is first transformed into the featurized data, and then passed to the ML algorithm to train a model.\n",
    "- By calling `predict`, the raw input data is first transformed into the featurized data (just like `fit`), and and then used to get predictions (using the model trained when we called `fit`).\n",
    "\n",
    "You can even create your own transformers and inculde them in a pipeline step, using as shown [here](https://scikit-learn.org/stable/modules/preprocessing.html#custom-transformers), but we won't worry about this for this assignment.\n",
    "\n",
    "Let's also have some data that we can use for scoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create local data folder to store\n",
    "data_folder = \"data\"\n",
    "os.makedirs(data_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./data/new_data.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/new_data.json\n",
    "{\"age\": {\"0\": 40, \"1\": 47},\n",
    " \"balance\": {\"0\": 580, \"1\": 3644},\n",
    " \"campaign\": {\"0\": 1, \"1\": 2},\n",
    " \"contact\": {\"0\": \"unknown\", \"1\": \"unknown\"},\n",
    " \"day\": {\"0\": 16, \"1\": 9},\n",
    " \"default\": {\"0\": \"no\", \"1\": \"no\"},\n",
    " \"duration\": {\"0\": 192, \"1\": 83},\n",
    " \"education\": {\"0\": \"secondary\", \"1\": \"secondary\"},\n",
    " \"housing\": {\"0\": \"yes\", \"1\": \"no\"},\n",
    " \"job\": {\"0\": \"blue-collar\", \"1\": \"services\"},\n",
    " \"loan\": {\"0\": \"no\", \"1\": \"no\"},\n",
    " \"marital\": {\"0\": \"married\", \"1\": \"single\"},\n",
    " \"month\": {\"0\": \"may\", \"1\": \"jun\"},\n",
    " \"pdays\": {\"0\": -1, \"1\": -1},\n",
    " \"poutcome\": {\"0\": \"unknown\", \"1\": \"unknown\"},\n",
    " \"previous\": {\"0\": 0, \"1\": 0}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run through the following steps to first refactor and test code:\n",
    "\n",
    "- **Step 1:** Compose the data processing and training steps in the above code into one pipeline as shown in the referenced doc. <span style=\"color:red\" float:right>[15 point]</span>\n",
    "- **Step 2:** Call `fit` and `predict` on the pipeline to make sure that it all works. Remember to pass them the **un-processed** (original) data, since the data processing should be built into the pipeline now. <span style=\"color:red\" float:right>[5 point]</span>\n",
    "- **Step 3:** Save your pipeline object using `joblib` as shown [here](https://sklearn.org/modules/model_persistence.html). <span style=\"color:red\" float:right>[5 point]</span>\n",
    "- **Step 4:** Now write a **new script** for scoring: it loads the pipeline you saved in the last step, reads the data `../data/new_data.json` and converts it to a `pandas.DataFrame` object, and obtains predictions on it. The predictions should be stored as a `json` file `../data/new_preds.json`. <span style=\"color:red\" float:right>[10 point]</span>\n",
    "\n",
    "To begin work on the assignemnt, it's best to first copy the training script in a cell below and begin modifying it according to the instructions in steps 1-3. Then create a new cell and populating with the scoring script described in step 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frontload all of the library imports\n",
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "# set global state variable\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start I will not include reading the csv into a dataframe and grabbing the numeric and categorical columns as this varies depending on the dataset. I would expect someone who is working with this model to do this manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to create functions to wrap in pipeline\n",
    "\n",
    "class PipelineHelper:\n",
    "    \n",
    "        \n",
    "    # generate full dataframe from csv, numeric and categorical column names\n",
    "    def generate_df_cols(csv_path, target):\n",
    "        \n",
    "        # read the csv into a dataframe\n",
    "        df = pd.read_csv(csv_path, delimiter = ';')\n",
    "\n",
    "        # separate numeric, categorical and target columns\n",
    "        num_columns = df.select_dtypes(['integer', 'float']).columns\n",
    "        cat_columns = df.select_dtypes(['object']).drop(columns = target).columns\n",
    "        \n",
    "        # print column types\n",
    "        print(\"Numeric columns are {}.\".format(\", \".join(num_cols)))\n",
    "        print(\"Categorical columns are {}.\".format(\", \".join(cat_cols)))\n",
    "        \n",
    "        return df, cat_columns, num_columns\n",
    "\n",
    "    # split the data with a 90/10 training vs test\n",
    "    def split_data(df, target):\n",
    "        # split the data into training and testing data as 90-10 split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df.drop(columns = target), df[target], \n",
    "                                                        test_size = 0.10, random_state = 42)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    # after training pipeline get the precision and recall score\n",
    "    def get_metrics(y_test, y_pred):\n",
    "       \n",
    "        precision_test = precision_score(y_test, y_pred, pos_label = 'yes') * 100\n",
    "        recall_test = recall_score(y_test, y_pred, pos_label = 'yes') * 100\n",
    "        print(\"Precision = {:.0f}% and recall = {:.0f}% on the validation data.\".format(precision_test, recall_test))\n",
    "        \n",
    "        return precision_test, recall_test\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created a python submodule with these functions that can be utilized for future use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns are age, balance, day, duration, campaign, pdays, previous.\n",
      "Categorical columns are job, marital, education, default, housing, loan, contact, month, poutcome.\n"
     ]
    }
   ],
   "source": [
    "# generate data needed pre pipeline\n",
    "pipe_help = PipelineHelper\n",
    "\n",
    "csv_path = \"./bank-full.csv\"\n",
    "\n",
    "df, cat_cols, num_cols = pipe_help.generate_df_cols(csv_path=csv_path, target='y')\n",
    "\n",
    "# get training and testing data\n",
    "X_train, X_test, y_train, y_test = pipe_help.split_data(df, target='y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frontload all of the library imports\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# set global state variable\n",
    "SEED = 42\n",
    "\n",
    "# # read the csv into a dataframe\n",
    "# bank = pd.read_csv('./bank-full.csv', delimiter = ';')\n",
    "\n",
    "# # separate numeric, categorical and target columns\n",
    "# num_columns = bank.select_dtypes(['integer', 'float']).columns\n",
    "# cat_columns = bank.select_dtypes(['object']).drop(columns = \"y\").columns\n",
    "\n",
    "# # print column types\n",
    "# print(\"Numeric columns are {}.\".format(\", \".join(num_cols)))\n",
    "# print(\"Categorical columns are {}.\".format(\", \".join(cat_cols)))\n",
    "\n",
    "# # split the data into training and testing data as 90-10 split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(bank.drop(columns = \"y\"), bank[\"y\"], \n",
    "#                                                     test_size = 0.10, random_state = 42)\n",
    "\n",
    "# create pre processing functions\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocess\", \n",
    "        ColumnTransformer([ # preprocess data at beginning of pipeline\n",
    "            (\"onehot\", OneHotEncoder(sparse_output=False), cat_cols),\n",
    "            (\"standardize\", StandardScaler(), num_cols)\n",
    "        ])\n",
    "    ),\n",
    "    (\"logit\", LogisticRegression(max_iter = 5000, solver = 'lbfgs', random_state=SEED))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocess&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;onehot&#x27;,\n",
       "                                                  OneHotEncoder(sparse_output=False),\n",
       "                                                  Index([&#x27;job&#x27;, &#x27;marital&#x27;, &#x27;education&#x27;, &#x27;default&#x27;, &#x27;housing&#x27;, &#x27;loan&#x27;, &#x27;contact&#x27;,\n",
       "       &#x27;month&#x27;, &#x27;poutcome&#x27;],\n",
       "      dtype=&#x27;object&#x27;)),\n",
       "                                                 (&#x27;standardize&#x27;,\n",
       "                                                  StandardScaler(),\n",
       "                                                  Index([&#x27;age&#x27;, &#x27;balance&#x27;, &#x27;day&#x27;, &#x27;duration&#x27;, &#x27;campaign&#x27;, &#x27;pdays&#x27;, &#x27;previous&#x27;], dtype=&#x27;object&#x27;))])),\n",
       "                (&#x27;logit&#x27;, LogisticRegression(max_iter=5000, random_state=42))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocess&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;onehot&#x27;,\n",
       "                                                  OneHotEncoder(sparse_output=False),\n",
       "                                                  Index([&#x27;job&#x27;, &#x27;marital&#x27;, &#x27;education&#x27;, &#x27;default&#x27;, &#x27;housing&#x27;, &#x27;loan&#x27;, &#x27;contact&#x27;,\n",
       "       &#x27;month&#x27;, &#x27;poutcome&#x27;],\n",
       "      dtype=&#x27;object&#x27;)),\n",
       "                                                 (&#x27;standardize&#x27;,\n",
       "                                                  StandardScaler(),\n",
       "                                                  Index([&#x27;age&#x27;, &#x27;balance&#x27;, &#x27;day&#x27;, &#x27;duration&#x27;, &#x27;campaign&#x27;, &#x27;pdays&#x27;, &#x27;previous&#x27;], dtype=&#x27;object&#x27;))])),\n",
       "                (&#x27;logit&#x27;, LogisticRegression(max_iter=5000, random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocess: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;onehot&#x27;, OneHotEncoder(sparse_output=False),\n",
       "                                 Index([&#x27;job&#x27;, &#x27;marital&#x27;, &#x27;education&#x27;, &#x27;default&#x27;, &#x27;housing&#x27;, &#x27;loan&#x27;, &#x27;contact&#x27;,\n",
       "       &#x27;month&#x27;, &#x27;poutcome&#x27;],\n",
       "      dtype=&#x27;object&#x27;)),\n",
       "                                (&#x27;standardize&#x27;, StandardScaler(),\n",
       "                                 Index([&#x27;age&#x27;, &#x27;balance&#x27;, &#x27;day&#x27;, &#x27;duration&#x27;, &#x27;campaign&#x27;, &#x27;pdays&#x27;, &#x27;previous&#x27;], dtype=&#x27;object&#x27;))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">onehot</label><div class=\"sk-toggleable__content\"><pre>Index([&#x27;job&#x27;, &#x27;marital&#x27;, &#x27;education&#x27;, &#x27;default&#x27;, &#x27;housing&#x27;, &#x27;loan&#x27;, &#x27;contact&#x27;,\n",
       "       &#x27;month&#x27;, &#x27;poutcome&#x27;],\n",
       "      dtype=&#x27;object&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(sparse_output=False)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">standardize</label><div class=\"sk-toggleable__content\"><pre>Index([&#x27;age&#x27;, &#x27;balance&#x27;, &#x27;day&#x27;, &#x27;duration&#x27;, &#x27;campaign&#x27;, &#x27;pdays&#x27;, &#x27;previous&#x27;], dtype=&#x27;object&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=5000, random_state=42)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocess',\n",
       "                 ColumnTransformer(transformers=[('onehot',\n",
       "                                                  OneHotEncoder(sparse_output=False),\n",
       "                                                  Index(['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact',\n",
       "       'month', 'poutcome'],\n",
       "      dtype='object')),\n",
       "                                                 ('standardize',\n",
       "                                                  StandardScaler(),\n",
       "                                                  Index(['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous'], dtype='object'))])),\n",
       "                ('logit', LogisticRegression(max_iter=5000, random_state=42))])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try to use this on X_train data\n",
    "X_train_processed = pipeline.fit(X_train, y_train)\n",
    "X_train_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/logit_pipeline.joblib']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save pipeline\n",
    "from joblib import dump, load\n",
    "import json\n",
    "\n",
    "dump(pipeline, './data/logit_pipeline.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for X_test\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.0 34.0\n"
     ]
    }
   ],
   "source": [
    "# get precision and recall\n",
    "# Show precision and recall for both training and testing predictions\n",
    "precision_test = precision_score(y_test, y_pred, pos_label = 'yes') * 100\n",
    "recall_test = recall_score(y_test, y_pred, pos_label = 'yes') * 100\n",
    "\n",
    "print(precision_test.round(0), recall_test.round(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logit = dump(logit_regressor, 'filename.joblib') \n",
    "# load json file\n",
    "new_data_path = \"./data/new_data.json\"\n",
    "pipeline_path = \"./data/logit_pipeline.joblib\"\n",
    "\n",
    "def load_df(json_path):\n",
    "    with open(json_path, \"r\") as j:\n",
    "        data = json.load(j)\n",
    "    new_df = pd.DataFrame(data)\n",
    "    return new_df\n",
    "\n",
    "logit_pipeline = load(pipeline_path)\n",
    "new_df = load_df(new_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>balance</th>\n",
       "      <th>campaign</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>default</th>\n",
       "      <th>duration</th>\n",
       "      <th>education</th>\n",
       "      <th>housing</th>\n",
       "      <th>job</th>\n",
       "      <th>loan</th>\n",
       "      <th>marital</th>\n",
       "      <th>month</th>\n",
       "      <th>pdays</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>previous</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>580</td>\n",
       "      <td>1</td>\n",
       "      <td>unknown</td>\n",
       "      <td>16</td>\n",
       "      <td>no</td>\n",
       "      <td>192</td>\n",
       "      <td>secondary</td>\n",
       "      <td>yes</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>no</td>\n",
       "      <td>married</td>\n",
       "      <td>may</td>\n",
       "      <td>-1</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>3644</td>\n",
       "      <td>2</td>\n",
       "      <td>unknown</td>\n",
       "      <td>9</td>\n",
       "      <td>no</td>\n",
       "      <td>83</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>services</td>\n",
       "      <td>no</td>\n",
       "      <td>single</td>\n",
       "      <td>jun</td>\n",
       "      <td>-1</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  balance  campaign  contact  day default  duration  education housing  \\\n",
       "0   40      580         1  unknown   16      no       192  secondary     yes   \n",
       "1   47     3644         2  unknown    9      no        83  secondary      no   \n",
       "\n",
       "           job loan  marital month  pdays poutcome  previous y_pred  \n",
       "0  blue-collar   no  married   may     -1  unknown         0     no  \n",
       "1     services   no   single   jun     -1  unknown         0     no  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "new_df['y_pred'] = logit_pipeline.predict(new_df)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save into new json file\n",
    "new_json_path = './data/new_preds.json'\n",
    "new_json = new_df.to_json(new_json_path, orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write dependencies to data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=\"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/conda.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $data/conda.yaml\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.11\n",
    "  - pip\n",
    "  - pip:\n",
    "    - scikit-learn==1.3.0\n",
    "    - pandas==2.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of assignment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
